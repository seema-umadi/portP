
1) what is difference between docker and kubernetes?
	docker is container platform and kubernetes is container orchestration platform. kubernetes is advance tool as in container. kubernetes have features like auto-scaling, auto healing and multiple host. which docker does not have. auto-scaling means if load is increasing on a container then kubernetes have a feature like auto-scaling which automaticaliy increase the containers, this feature is not present in docker. 2nd is auto healing. docker container are emphemerial in nature thats why if any container goes down then application is also goes down. this is major drawback in docker but in kubernetes it have a auto healing quality, if container goes down then kubernetes automatically create another pod. 


1) what is the commands to view crontab linux?
	crontab -l

2) what is alias in linux?
	in linux alias is user-defined command which is a shortcut or subtitution for long command. syntax alias cron='crontab'

3) what chmod command do?
	chmod command is used to change the permission of files and directory like read,write,execute.

4) what is ssh port forwording?
	ssh port forwording is also known as a ssh tunneling, is the process of transmiting the data over an encrypted secure shell connection between a local and distant server.

5)what is zombie process?
	The process that have completed their execution, but their entries are not removed from the process table.

6) how to check the top cup consuming process?


7) what is blue-green deployement?
	blue-green deployment is deployment strategy , where two separate environments are created but identical. one environment is running current version of application i.e blue. and other environment is running a new version of application i.e green.

********************************************************************************************************************************************************************************************
what is kubernetes?
	k8s is a orchestration platform. where applications are running as a container.k8s has a multiple feature which docker does'nt have like auto scaling,auto healing and enterprise level support. we create a clusters and in that cluster we run our application. 

2) what is k8s service what are the type of service?
	the idea of service is to group a set of pods endpoint into a single resource. you will get a stable ip address, using that stable ip address internal clinet in cluster can communicate with pods in service. every pod has a ip address associate with them. in deployment pods can come and go , and there ip address are changed. so using pods ip directly doesn't make sense.with a service you get a stable ip address that last for life of service. request are balanced across the pods in service.
type of service:
1)clusterIp= internal client send a request to cluster's stable Ip address.
2)nodeport= client send a request to ip address of node. one or more nodeport valuees are specified by service
3)load-balncing=client send a request to ip address of load-balancer.
4)external= client send a request to dns name of service.
5)headless= headless is a service where you want group of pods and not the stable ip address.

3) how do you handle k8s cluster security?
	we have done lot of thing for security of cluster. we have secure secretes. we have secure pod communication using egrees and ingress. we also use Rbac policy. rbac stand for role based access control. which define which user has what amount of access to the resource. for multiple projects and multiple people use same k8s cluster, we have used namespaces to enable multi-tenancy support.audit logging enable to monitor k8s security.

4)you have an application deployed on k8s that is experencing increased traffic,how would you scale the application to handle the increased load?
	first identify bottleneck, i will check cluster to analyse resource including cpu, memory and networking to determine a limited factor. once i know this, if bottleneck is cpu and memory i would scale application horizontaly, using horinzontal pod autoscaler, if bottleneck is resource specific i would vertically scale the application by upgrading resources allocated to pod. other than this i will scale the nodes using node auto scaler.
	
5) while troubleshooting a networking issue in the cluster,you noticed kube-proxy in the logs.what is the role of kube-proxy in cluster?
	kube-proxy is a componant of k8s that run on each node. it handles tcp/udp packets forwording between backend network services.it crucial for communication between pods and service by routing traffic to the right destination. 

6)your team is planning a high availablity kubernetes cluster.describe the process and consideration for desiging and high availibity k8s cluster.
	To creat a cluster on high availabilty we need to deploy multiple nodes and pods in multiple daisis, we need to setting up multi-master nodes. to create this we need to deploy master nodes in three or more daisis to ensure redundancy and fault tolerance. we need to distribute etcd members across differnt availiblity zone as simliar as master node. to ensure data redundancy and zone failure. apart from this we need to configure load balancer such as aws's network load balancer to evenly distribute API request among API server. this setup eliminate the risk of single API server becoming bottleneck and point of failure. along with this we can enable node auto repair feature by k8s engine which automatically detect and replace unhealthy nodes. maintaing the descired cluster size and health without manual intervention.
  
7)what happens when the master or the worker node fails?
	master node is something which handle evrything inside the cluster. if master node is fails then cluster continusly operates but pod managemnet is lost. there is no new pods are being created. on the other hand worker node is something application is running on the worker node. if worker node is failed then obvisouly application running on that node will also fail. you will get dns failure on worker node. but master node is working properly, k8s noticed the worker node is failed then it find out worker node is not ready so that new pod is not going to create on that node. and schedular is removed all pods which are on failed worker node and launch it somewhere else on other node. so this is happens when master node and worker node are failed. 

8)how does ingress help in k8s?
	ingress is a object in k8s, which is used to expose the service to the external world. ingress expose service using single ip address or dns name. also ingress provides feature like load balncing, ssl termination. it simplify the management and configuration of your service.

9) what do you know about headless service?
	headless service is one of the service type in k8s. where the service type is clusterIp but there is no stable ip associate to the service. so here the spec clusterip set to none, and the spec type is clusterIp.headless service can be used in database cluster, with each replica of the database having its own ip address.this allow for direct communication to pods.

10)what is init container, why we need?
	on the pod there are 2 containers are running one is the init container and other is application container. init container is run before the application container. there are multiple use cases for init container including runing a script, setting up networking configuration or download confiuration file or anything that need to run before application starts. the purpose of init container is perform initialization task or set up procedure which is not present in application container image. 

11) critical application running on one of the node is not working properly.how do you monitor application in k8s?
	we can monitor k8s cluster using prometheus and grafana , we can use other different tool as well. also we can run kubectl top and kubectl stat command to monitor k8s cluster. we can create alarting using promethues alrt manager.

12)your manager read an artical on gitops and want you to do POC on it. what is gitops and how do you impliment?
	gitops is somethig where you need to write a manifest file and push into the repository. once file is pushed. it automatically detect the changes in repository and perform the task according to the changes automatically without users manual intervention.

13)explain RBAC?
	RBAC stands for role based access control, where we define which user has what amout of access to the resource. it is a main componant to maintain security in cluster. it defines access like deployment,manifest, services. let's assume one user has a access to deployment but he or she does not access pode and nodes.	

14)how do you perform maintainance on the k8s node?
	to perform maintainance on k8s node, first we need to remove running application on the node. so you first make sure node unschedule which means there is no new pod should running on the node. you can do this using kubectl cardon command and remove existing port using kubectl drain comand.then perform maintaince , optionaly you can reboot node. once this is done make sure pods are deployed on node again. and you can check the status of pod using kubectl get node and kubectl describe command.

15)explain daemonset
	daemon set is let you run pods on all cluster node. they are used on system level task like logging and monitoring nodes.

16)differnce between configmap and secrets?
	configmap is used to save the application configuration in plaine text. where secrets are used to save secreate credential like databases, username, password in encrypted format.

17)what is purpuse of oprators?
	operators is the method packing the complex application on k8s cluster. it is used to packing,deplying and managing k8s application. oprater uses k8s API to automate task such as deployment, scaling, implimenting custom controller. 

18)how can you run a pod on specific node?
	we can run pod on specific node using node affinity or node selectors. and you can also use node name. label the node using lable.

19)suppose a pod exceed its memory limit. what signal will be sent to the process?
	if pod exceed its memory limit kernel's out-of-memory killer send a signkill single to pod process, terminating it immediatly. 

20)what will you do to upgrade k8s cluster?
	k8s come with new upgradation every 4 months. to upgarde cluster we follow offical documentation in which we followed steps like first we need to upgrade master node and then worker node. after the upgradation is need we need to verify cluster is running properly or not.

21) kubernetes imagepull policy?
	k8s always use latest image there are three imagepull policy.
1)ifnotpresent= image is pulled only when images is not present locally.
2)never= kubelet does not try to fetch image if image is present somehow in anywhere locally, it directly start the container.
3)always= 


22)company is migrating its monolithics application to a micro-service architecture on kubernetes. how would you plan and excute this migration?
	access the monolithic application to identify micro-services. define container images and k8s deployment configurations. setup a k8s cluster on-primises or using cloud provider. deploy micro-services to the cluster with proper networking. implimented moniotring and logging solutions, decompose the monolithic app into micro-services gradually.

23) you have a stateful application that requires persistent storage in k8s. how would you configure persistent storage for this application?
	define persistentvolume(pv) nd persistentvolumeclaim (pvc)in yml.choose appropriate storage class. specify access mode,capacity and other parameters in vpc. attach pvc to pods in their yml manifest. deploy pods to k8s cluster, ensuring access to persistent storage.

24)you're experiencing performace issue with your k8s cluster. how would you diagnose and resolve these issue?
	 monitor resourse utilization: cpu,memory and disk. use k8s dashboard or monitoring tools. check logs of indiviual pods for errors. scale up cluster by adding more worker nodes if needed. optimise resources request and limits for pods. implement horizontal pod autoscaling.

25)you need to deploy a k8s application across multiple environment (dev,staging,production) with different configuration. how would you manage enviornment specific configuration in k8s?
	use configmap to store envirment specific config data. create separate configmap for each enviorment. reference configmap in pod deployment configs using env vaiable or volume mounts.use k8s namespace to isolate resources. utilize tools like helm for templating and managing manifest.

26)your team want to impliment rolling updates for a k8s deployment to minimize downtime during application upgrades. how would you achieve this?
	define new version of the container image. update the deployment configuration to use the new image. set deployment strategy to "rollingupdate". apply updated deployment config to the cluster. monitor rollout progress and rollback if needed.

27)What is Kubernetes, and why is it important in the world of container orchestration?
	k8s is container orchestration platform, it is a advance tool, which overide th problems of docker. it has a multiple advance feature like auto-scaling, auto-healing, provide enterprise level support. in docker we face the issue of container goes down, this problem is solved in k8s, if any pod goes down it automatically create another pod. it also provide namespace which is like a logical partition within the cluster, which allow you to divide cluster resources among multiple users or teams.in docker we deploy appication in conatiner in k8s we have clusters and within cluster we have pods and in that pods we dploy containerised application.

28)Explain the key components of Kubernetes and their roles in container management.
	there are main two key componat. 1)control plane(master node) 2)data plane(worker node) if i talk about control plane there are 4 objects kube-api server, kube-controller manager, kube- schedular, etc.
kube-api server=it is a componate of k8s control plane which expose the k8s api.it is like a inital gateway to cluster, that listens to updates or quresie via cli kublet. which interct with api server and tells what things need to be done like creating pod or deleting pod. it works like gatekeeper, it generally vaildate request recived and forword to the schedular or other resources to perform task. request can not direcly goes to cluster.
kube-schedular=kube api server recive a request to schedule pod that is passed to the schedular. it intelligently decide on which node schdule a pod for better efficency of cluster.
kube-controller-manager-this is responsible for runing contoller and handle various aspects of clsuter control loop. kube controller include replica controller which ensure desired number of replica at a given application running where as node controller ensure it markes as ready or not ready according to the current state of node.
etcd-it store the key-value pair information.also store the cluster state changes in etcd. 
data plane(worker node)
container runtime- container runtime needs run application container running on pod
kublet-it intract with container runtime and worker node, it is responsible for starting pod
kube-proxy-it handles tcp-udp packet forwording to backend network services. it establish communication between service to pod, and route traffic to right destination.

29)How do you deploy a containerized application on a Kubernetes cluster? Walk me through the process.
	first we need to ensure we have access to the k8s cluter, this can be set up locally using minikube or kind etc, or using cloud providers k8s service like aws eks other cloud provider also have this type of service but i work on eks. for locally we need to install minikube or kind and start. cluster is going to create. for containered application we need to write dockerfile from that docker we build an image using docker build command. then we push this command in docker registry. and we need to installed and configure kubectl command tool for k8s. then we need to write k8s manifest yml file for deployment and service. service file we need beacuse of service type and stable ip adrress. because in in deployment many pod can come and go, and the ip adress of pods are changed so directly use pods ip adress does not make sense. so need stable ip adres which service is provide. after writing a yml files we need to apply using command kubectl apply -f and filename. to verify pods are running properly we can use command kubectl get pods.our containerised application is running on cluster. 

30)Describe Kubernetes Deployments and StatefulSets. What are the differences, and when would you use one over the other?
	deployments are designed for stateless application. they can manage stateless pod and ensure desired number of replica are running and can updating and rollback is seamlessly. pod are interchangable and does not retain stable identity or state. ensuring rollback to previous version if updated fails. and rolling updates can be done with no downtime update. use case of stateless are web server, api server , frontend application or any application in which pods statble identity or persistent storage does'nt matter.
statefullset are designed for statefull application, which need persistent storage and ensure pods has unique and stable identity and consistent storage. each pod has its own storage, and it attached to the pod even if pod is resucheled. uses case of this is a databases or any other application that need stable identity or persistent storage.

31)How does Kubernetes handle load balancing for containerized applications?
  	k8s have services which handle the laod balancing for containerised application. there are differnt types in service 1)clusterip- internal client send the request to the cluster ip and it route the traffic to pods within cluster. 2) nodeport= internal client send a request to port of node, which route the traffic to the multiple node. 3) loadbalancer- it is a external load balancer which is provided by cloud providers like aws elb there are other cloud providers as well but basically i work on elb. it route the traffic to nodeport. 4)dns- this route the traffic based on dns queries. 

32)What is a Kubernetes Namespace, and why would you use multiple namespaces in a cluster?
	k8s namespace is logical partition within cluster, which allow to divide cluster resources among k8s multiple user and groups. this isolation is useful for mananging, controlling the cluster. it is used for large cluster, namesapce also can be a team or a project.rbac is applied on the namespace level. in which we specify which user has what amount of access to cluster. we can create multiple namespace like for various envornment like dev,stage and producation. to create name we can use command kubectl create namespace and the name.
	
What is Kubernetes' role in auto-scaling, and how can you set up Horizontal Pod Autoscaling (HPA)?
	you can automatically scale workload using horizontalpodautoscale. it implimented on kubernetes api server and controller, and periodically adjust the number of replica in a workload match observed resource utilization such as cpu and memory. we can scale the pod based on cup,memory utilzation, using kubctl command.   

34)Explain Kubernetes' role in self-healing and how it handles container failures.
	k8s provide feature called auto-healing. k8s uses probes to check the helath of container, liveness probe determine container is running and readiness probe checks container is ready to serve the traffic. if pods or container fails, k8s restart it. it is managed by kublete, this agent is run on every node.it is responsible for start pod. replicaset is also like a auto healing, we define a desired state of application in replicaset, replicaset controller ensure to maintain the desired state. if pod fails it restrt the pod. also if node is unresponsive, k8s mark it unavailble and pods are reshecdule on healthy node. 

35)How would you upgrade a Kubernetes cluster to a new version while minimizing downtime?
	first review the release note to review the changes and new feature. then take a backup of current version etcd, stored cluster data. take a backup of critical application. then drain the master to ensure no new pods are created during upgradation. upgrade the k8s control plane, k8s kubelet,kubctl and other things, and then uncardon the master node. same for the worker node.verify nodes are in ready condition check the status of nodes and pods. monitor logs.
	
36)Explain Kubernetes RBAC (Role-Based Access Control) and how you would configure it to secure your cluster.
	RBAC is role based access control which is used to define which user,group have what amount of permission to the resource like,pods,deployments, services. to configure rbac we need to write manifest file for role or cluster role, to define permission required and create appropriate role or clusterrole, and then configure role binding or cluster role binding to bind role or cluster role to user,group or namespaces. then regularly audit the rbac policies.

37)How do you handle secrets rotation for applications running in Kubernetes, and why is it important?
	k8s provide us screte managment,for storing confidencial data. without exposing any where we can use variables for credencial. to cretae screrates we use kubctle command we define credential name like mycredential then provide username and password. this credential are used in yml manifast file, in that we use credential name and variables of username and password. so here w dont need to expose the id, password it is most imp to securure our cluster.

***38)Discuss the challenges and best practices for running stateful applications in Kubernetes, such as databases.
	stateless appication can easily scale and recover by redeploying pods, where stateful application require consistent and realible storage. for that we can use k8s persistentvolume and persistentvolume claims to manage storage.using storage class we can automate the creation of pvs. for network configuration, we need pods unique identifier and stable storage. scaling stateful application is more complex than stateless because databases need a persistent storage. we can horizonatly scale like adding more replicas and vertically scale adding more resources to the instance should be managed carefully. for database, consider using database-specific solution like read replica, partitioning.performing upgrades and mainataince without downtime is also a challenge for stateful application. for that we need rolling updates for controlled and sequencial upgrade. for diaseter recovery we need to impliment regular backups and monitoring using tools like prometheus and grafana.

39)Your team has developed a new version of an application that you need to roll out to a Kubernetes cluster without affecting the existing users. Describe the strategy and steps you would take to perform a zero-downtime deployment
	there can be 2 strategy are used one is green-blue deployment and another is canary deployment. in blue-green deployment we have two seprate env but identical. our application is running on blue so we need to create one more env called as green where nw verion of application is deployed.both env running parallel . make sure all configuration should be same for both env. use labels for blue and green pods. test the env continuly, and internal testing must be done. apply monitorig tool to ensure appplication is running smothly once you confident about new env then route traffic to this. test the application. once you confident about is then decommison the blue env, take backup as well. before decommission. now green env is considered as blue. you can work for new env deployment cycle. where as canary strategy is you deploy your application on small amount of pods. test the application by rounting to specfic percent of user, both env running parallel. if application is stable then increse percentage to route traffic to canary pod. check metrix,logs for error, monitor the health of application. once everything is running smothly the route all traffic to canary pods. by scaling down old pods and scaling up canary pod. if any issue are occured then again scale down the carany pods and scale up the old pods.

40)Your organization uses multiple Kubernetes clusters across different cloud providers and on-premises data centers. How would you implement a multi-cluster strategy to manage and orchestrate containers seamlessly across all clusters?
	ensure single control plane to manage multiple cluster, we use kuberenetes fedration. by using kubefed command we can set k8s fedration single control plane. and deploy resources using kubefed

41)You want to enable secure communication between services in your Kubernetes cluster. Describe how you would configure and manage network policies for pod-to-pod communication.
	to ensure communication between servies in k8s cluster and manage network policies for pod-to-pod communication you need to configure k8s network policy. to define network policy we can use networkpolicy resource. for that we need to write yml manifest file where we define kind as a network policy and we use namespace and define ingress and egress rules. and then we need to apply this file using kubectl apply command. test the deployed application, is pods are allowed to communicate each other, to check we can use wget or curl command. 

42)One of your Pods is experiencing high resource utilization and affecting other Pods on the same node. How would you diagnose and address this issue, ensuring resource isolation?
	check pods resource usages by using command kubectl top pod, check which pod faces high resource utilization. you also check logs using command kubectl logs, and for relevant error or warning use kubectl describe pod command. impliment monitoring tool for monitor resource utilization which check the matrix. ensure the resource request and limits are set. we can use horizontal pod autoscaller to handle resource utization. for that we need to write yml manifest file in which we set the rule of minimum replica set and maximun replica set. also we write the condition for cpu utilization or memory utilzation where we define the avarage utilaztion. this is how it can handle the high resourse utilization. also we can use pdb which is pod distribution budget, which maintain minimum no. of pod available for disruption.

43)You have a stateless application with variable traffic patterns. How would you configure Horizontal Pod Autoscaling (HPA) to automatically scale the application based on resource utilization?
	horizontal pod auto-scaler relies on matrix server to collect matrix resources. for that you need to set up matrix server into the k8s cluster server. then you can use hpa.
   
44)You need to migrate an existing monolithic application to a microservices architecture running on Kubernetes. How would you plan and execute this migration while minimizing disruptions?         
	first we need to access monolithic application for identify functinality,dependency and to identify micro-services. create smaller micro-services from monolathic application, for that we need to setup a cluster. and run ci/cd pipeline for biuld test and deploy the application. first deploy monolathic application on kubernetes. then create a separeate codebase for every micro service and containerize the application using docker and deploy on kubernetes cluster.

45)You are tasked with setting up a disaster recovery plan for your Kubernetes cluster. Describe the strategies and tools you would use to ensure data and application availability in the event of a cluster failure.
	for that we can set up multi-region, deploy k8s cluster in multi region fo availibilty and provide global load balancer like aws's global accelerater to manage traffic, and route traffic to correct location. also take volume snapshot, take backups on regular basis. monitior the cluter use probes for ensuring helath of container, provide alatring if any problem is accured for based on resource utilization.

46)You want to implement RBAC (Role-Based Access Control) in your Kubernetes cluster. Explain how you would define roles, role bindings, and service accounts to secure your cluster.
	RBAC is used to define which user has what amount of access to the k8s cluster like pods,services and deployments. first we need to create role, we write a manifest yml file for role. and set kind as a role,set name of the role. provide a particular namespace. it is important to create role. then define a permission in the rules section. we define resourece like pod,deployment etc and define verbs which means what action can be performed. rolebinding grants the permission in the role to the set of user, we write a yml file for rolebinding, set kind as a rolebinding, specify name space and name the rolebinding, then we define subject which means list of users. then add role related info like kind set to role and name of the role which we specify in role file. and then apply this yml file to the cluster. this how we use RBAC policy.
	
47)Your Kubernetes cluster is running out of resources, and you need to optimize resource utilization. Explain the steps you would take to right-size and optimize resource allocation for your workloads.
	first we need to monitor the k8s cluster. we set the monitoring tools like prometheus and grafana to collect the matrix of cpu and memory utilization. also we can use command like kubectl top for resource utilzation. we need to anaylse the resources. we can use horizontal pod autoscaler if bottelneck is cpu or memory utilzation. we need to write a yml file for HPA. we set kind as a horizontal pod autoscaler. we set a minimum and maximum replica set according to the utilzation wehich define in same file like resouce type is cpu or memory also we set avgutilization. this is how we can scale the applicaion horizonatly also we can scale the cluster verticaly, vertical auto scaler is automatically adjust the cpu and memory reservation for pod, which help us for right-size pod. also we can use node auto scale to increase the no. of nodes if needed. 	

48)Your organization is adopting GitOps for managing Kubernetes configurations. Describe the GitOps workflow and the tools you would use to implement it.
	gitops is a basically used for automate the continues deployment.for that we need to write configuration file and push to the github repo and impliment cicd pipeline. if any changes are happend in git so it automatically detect the changes and perform the task accordingly. i use agrocd tool which is based on gitops principle. it take care of cd part, and we need to do continuews integration part. argocd is not responsible for ci part. we need to install argo cd on our server and provide namespace. it will provide a dashboard, in that we define our project. it match the current state and desired state according to the configurtion file, also if we change any of our source code it detcet that chnages and if code is passed on cicd pipeline argo cd automatically deploy the application.	 

You are troubleshooting a performance issue in a Kubernetes cluster. Walk me through the steps you would take to identify the root cause and optimize the cluster's performance.
	for that first we need to monitor the cluster using monitoring tools like prometheus and grafana. prometheus is lies on api server and collect the matrix of resource utilizaion. and grafana provide visulation through the cluster or resource utilzitaion. if required then we can add more nodes. or implimetn hpa or vpa as well. check logs for clear error msg where is problem encounterd.

how can a service know which pod to send traffic to in k8s?
	service uses label and selector to determine on which pod traffic routed to.while creating service, we configured label and selectors of target pods.kubernetes maintain the list of endpoints that matches label and selectors of the service. this list is dynamically updated when pods are added or removed.kube-proxy watches the kubernetes api for changes in service and route traffic to right destination. 

if you dont set service in k8s then how can a request reach the app?
	request can reach the app using direct pod ip. in k8s cluster every pod has a unique ip address if we dont use service request routed to pods ip. but it is not good practise specially for production. because pods are ephemerial in nature then can come and go , restrt then ip address got changed everytime. thats why its doest make sense using direct pod ip. also does not provide scaliblity and load balncing. fpr that we need a stable ip to communicate with pod.

how can you deploy in k8s using aws?
	u can use aws ui or cli as well for creating k8s cluster. install kubectl k8s cli, and eksctl for creating and managing cluster. you need to create cluster and add nodes to the cluster. then write deployment.yaml file for deployment which include lable and selectors, image name etc. then apply using kubectl apply depolyment.yaml and write service.yml for service which include selector and service type protocol, port etc. apply using command. if you mention service type as a loadbalncer then aws provision load balancer, we can access appliction using external ip address, to know this ip we can excute cmd like kubetl get service (name) my-app-service. we will find ip address. using load balancer service type is best practice for production.

if you are using eks then how can a service in aws talk with another service in aws?
	there are sevreal methods if the requirement is service to service communication within eks cluster.then we can use service types like clusterip where service is exposed internally to cluster ip, and traffic routed to cluster ip address, using nodeport service, service expose to node's ip address, and traffic routed to nodes ip. loadbalncer provide external ip address to route traffic using this service types we can ensure service to service commumnication with eks cluster, if the requirement is service to service communication across aws service. we can setup vpc peering between eks vpc and the other service vpc. make a vpc peering connection between them configure route table. or we can use endpoint as well which is a secure and private connection between services without exposing to internet.  

scrum = update
shell scripting task = repitative task such as developers environment server we stopped at night time daily, so for that we wrote a script for shutdown and apply cron and  
wrote a script for whatever email alerts we have set, that need to be count daily and put in single sheet and apply cron. 
challenge= for some resons servers jvm thread went high for that we redeploy server , launch new servers using cicd.  
daily job, attaing scrum, creating cicd pipeline, monitoring


  
	





